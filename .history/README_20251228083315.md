# Lunar Lander Behaviour Cloning Toolkit

Utilities and training scripts I built while experimenting with Behaviour Cloning and DAgger on Gymnasium's "LunarLander-v3". The repository keeps everything needed to fine-tune a Deep Q-Network (DQN), generate rollouts for imitation learning, and recreate the plots that appeared in my project report.

<p align="center">
  <img src="docs/media/lunar_lander_trained.gif" alt="Animated landing demo">
</p>

## What I Added

- Upgraded the environment to Gymnasium `LunarLander-v3` with proper terminated/truncated handling.
- Enabled CUDA-aware training and inference so the models move to GPU automatically when available.
- Integrated TensorBoard logging (loss, reward, episode length, epsilon) plus CSV exports for offline analysis.
- Created `dqn_inference.py` to render trained agents, save state/action rollouts, and support downstream cloning experiments.
- Wrote `docs/graphing.py` to rebuild the smoothed reward and loss figures used in presentations.

## Quick Start

```bash
git clone https://github.com/bk-poudel/Lunar-Lander-Behaviour-Cloning.git
cd Lunar-Lander-Behaviour-Cloning
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Training Loop

- Entry point: `main.py`
- Algorithm: vanilla DQN with soft target updates (`tau=0.005`) and AdamW optimizer
- Replay buffer: 10k transitions sampled in batches of 128
- Exploration: epsilon decays from 1.0 to 0.01 (0.995 per episode)
- Gradient stabilisation: in-place clipping at ±100

Run training and follow metrics in TensorBoard:

```bash
python main.py
tensorboard --logdir=runs
```

The best checkpoint is saved to `models/dqn_lunar_lander.pth`.

## Inference & Rollout Collection

Render a trained agent and capture trajectories with:

```bash
python dqn_inference.py
```

The script loads the saved checkpoint, selects greedy actions, records rewards in the console, and writes optional rollout arrays to `inference_states.npy` / `inference_actions.npy`.

## Plotting Pipeline

1. Export TensorBoard scalars to CSV (or copy the provided ones in `docs/data/`).
2. Run `python docs/graphing.py` to produce smoothed reward and loss plots in `docs/media/`.
3. Embed the refreshed PNGs directly into reports or slides.

## Repository Layout

- `main.py` – DQN trainer with TensorBoard logging
- `dqn_inference.py` – greedy evaluation loop with optional rollout export
- `dqn.py` – three-layer fully connected policy network
- `docs/data/` – TensorBoard-exported CSVs from my best run
- `docs/graphing.py` – plotting helper that reproduces presentation-ready figures
- `models/dqn_lunar_lander.pth` – trained checkpoint packaged with the repo

## Credits

This toolkit sits on top of the excellent groundwork laid by [`wtcherr/lunar-lander-dqn`](https://github.com/wtcherr/lunar-lander-dqn). Their implementation provided the original DQN baseline; everything here reflects the extensions, data exports, and tooling I added for Behaviour Cloning and DAgger studies.
