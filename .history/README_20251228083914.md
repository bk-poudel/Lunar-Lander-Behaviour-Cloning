# Behaviour Cloning for Lunar Lander DQN

This project implements Behaviour Cloning using a pre-trained DQN as the expert policy for the Lunar Lander environment.

## Overview
- **Expert**: A DQN network trained by [wtcherr/lunar-lander-dqn](https://github.com/wtcherr/lunar-lander-dqn) on Lunar Lander.
- **Golden Dataset**: Collected by running the expert for 250 episodes, capturing state-action pairs.
- **Student**: A feed-forward MLP trained for 100 epochs to imitate the expert's actions.
- **Result**: The cloned policy performs comparably well on the environment.

## Project Structure
- `data/`: Golden Dataset from the DQN expert (state-action pairs).
- `models/`: Saved weights for the DQN expert and the trained MLP student.
- `scripts/`: Training and evaluation scripts for behaviour cloning.
- `configs/`: Hyperparameters and environment settings.

## Setup
1. Create a virtual environment (optional) and install dependencies:
  ```bash
  pip install -r requirements.txt
  ```
2. Place the expertâ€™s trained DQN weights under `models/`.


## Training
Train the MLP student on the Golden Dataset:
```bash
python3 BC_lunar_lander_train.py
```
Adjust hyperparameters via `configs/` or CLI arguments.

## Evaluation
Evaluate the cloned policy:
```bash
python scripts/evaluate.py --model_path models/mlp_student.pt --episodes 20
```
This runs the student for multiple episodes and reports average scores.

## Notes
- The expert DQN is considered the reference policy; the student MLP learns a supervised mapping from states to actions.
- Extend or fine-tune the student architecture via `configs/` to experiment with different hidden layers or activation functions.

## License
This project is distributed for educational and research purposes. Customize or extend as needed.